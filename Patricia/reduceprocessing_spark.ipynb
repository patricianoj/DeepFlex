{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lag, udf, greatest, least, struct\n",
    "from pyspark.sql.types import *\n",
    "import random\n",
    "import scipy.signal as signal\n",
    "import scipy.io\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Explore Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform EDA & Pre-processing on random sample of 3 activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load exercise dataset \n",
    "exercise_dataset = scipy.io.loadmat('../bigfiles/exercise_data.50.0000_singleonly.mat', struct_as_record=False)\n",
    "\n",
    "# Load activities and data full objects\n",
    "exercise_constants = exercise_dataset['exerciseConstants'][0][0].activities\n",
    "subject_data = exercise_dataset['subject_data']\n",
    "\n",
    "# extract activities names into an array\n",
    "all_activities = []\n",
    "for act in exercise_constants[0]:\n",
    "    all_activities.append(act[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_to_remove = ['Tap Left Device', 'Tap Right Device', 'Device on Table', \n",
    "'Non-Exercise', 'Note', 'Unlisted Exercise', 'Rest',\n",
    "'Arm Band Adjustment', '<Initial Activity>', 'Invalid', 'Arm straight up', \n",
    "'Tap IMU Device', 'Triceps extension (lying down) (left arm)',\n",
    "'Triceps extension (lying down) (right arm)', 'Alternating Dumbbell Curl']\n",
    "\n",
    "# Remove activities from all_activities that aren't exercises or have too <2 sets in the dataset\n",
    "for activity in activities_to_remove:\n",
    "    all_activities.remove(activity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dip', 'Overhead Triceps Extension', 'Band Pull-Down Row']\n"
     ]
    }
   ],
   "source": [
    "# Only take data from 3 random exercises as POC of this data processing pipeline\n",
    "activities_to_process = random.choices(all_activities, k=3)\n",
    "print(activities_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries for accelerometer and gyroscope data\n",
    "activities_accelerometer_data_dict = { activities_to_process[0]: [], activities_to_process[1]: [], activities_to_process[2]: []}\n",
    "activities_gyroscope_data_dict = { activities_to_process[0]: [], activities_to_process[1]: [], activities_to_process[2]: []}\n",
    "rep_counts_actual = []\n",
    "\n",
    "# iterate over subject data to search for those activities and save the data related\n",
    "for data_item in subject_data:\n",
    "    for x in data_item:\n",
    "        if len(x) > 0:\n",
    "            if x[0] is not None and len(x[0]) > 0:\n",
    "                data_activity_name = x[0,0].activityName[0]\n",
    "                data_activity_reps = x[0,0].activityReps[0]\n",
    "                data_item_accelDataMatrix = x[0,0].data[0,0].accelDataMatrix\n",
    "                data_item_gyroDataMatrix = x[0,0].data[0,0].gyroDataMatrix\n",
    "                if data_activity_name in activities_to_process:\n",
    "                    activities_accelerometer_data_dict[data_activity_name].append(data_item_accelDataMatrix)\n",
    "                    activities_gyroscope_data_dict[data_activity_name].append(data_item_gyroDataMatrix)\n",
    "                    rep_counts_actual.append(data_activity_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Library/Frameworks/Python.framework/Versions/3.8/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.1.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "23/06/27 10:13:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'elementType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      4\u001b[0m sc\u001b[39m.\u001b[39msetLogLevel(\u001b[39m\"\u001b[39m\u001b[39mERROR\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[39m# Create schema for signal data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m schema \u001b[39m=\u001b[39m StructType([\n\u001b[1;32m      8\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39moverall_set_num\u001b[39m\u001b[39m\"\u001b[39m, IntegerType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m      9\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39mactivity_name\u001b[39m\u001b[39m\"\u001b[39m, StringType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     10\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39mactivity_set_num\u001b[39m\u001b[39m\"\u001b[39m, IntegerType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m---> 11\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39mx\u001b[39m\u001b[39m\"\u001b[39m, ArrayType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     12\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39my\u001b[39m\u001b[39m\"\u001b[39m, ArrayType(), \u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m     13\u001b[0m     StructField(\u001b[39m\"\u001b[39m\u001b[39mz\u001b[39m\u001b[39m\"\u001b[39m, ArrayType(), \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     14\u001b[0m ])\n\u001b[1;32m     16\u001b[0m \u001b[39m# Initialize empty dataframes\u001b[39;00m\n\u001b[1;32m     17\u001b[0m gyroscope_df \u001b[39m=\u001b[39m ss\u001b[39m.\u001b[39mcreateDataFrame(sc\u001b[39m.\u001b[39memptyRDD(), schema)\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'elementType'"
     ]
    }
   ],
   "source": [
    "# Turn activities_gyroscope_data_dict and activities_accelerometer_data_dict into a spark dataframe\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Create schema for signal data\n",
    "schema = StructType([\n",
    "    StructField(\"overall_set_num\", IntegerType(), True),\n",
    "    StructField(\"activity_name\", StringType(), True),\n",
    "    StructField(\"activity_set_num\", IntegerType(), True),\n",
    "    StructField(\"x\", ArrayType(), True),\n",
    "    StructField(\"y\", ArrayType(), True),\n",
    "    StructField(\"z\", ArrayType(), True)\n",
    "])\n",
    "\n",
    "# Initialize empty dataframes\n",
    "gyroscope_df = ss.createDataFrame(sc.emptyRDD(), schema)\n",
    "accelerometer_df = ss.createDataFrame(sc.emptyRDD(), schema)\n",
    "gyroscope_data = []\n",
    "accelerometer_data = []\n",
    "\n",
    "# Iterate over activities and append data to dataframes\n",
    "overall_set_num = 0\n",
    "for activity in activities_to_process:\n",
    "    for activity_set_num, activity_set in enumerate(activities_gyroscope_data_dict[activity]):\n",
    "        x = [time_point[1] for time_point in activity_set]\n",
    "        y = [time_point[2] for time_point in activity_set]\n",
    "        z = [time_point[3] for time_point in activity_set]\n",
    "        data_row = (overall_set_num, str(activity), activity_set_num, x, y, z)\n",
    "        gyroscope_data.append(data_row)\n",
    "        overall_set_num += 1\n",
    "    overall_set_num -= (activity_set_num+1)\n",
    "    for activity_set_num, activity_set in enumerate(activities_accelerometer_data_dict[activity]):\n",
    "        x = [time_point[1] for time_point in activity_set]\n",
    "        y = [time_point[2] for time_point in activity_set]\n",
    "        z = [time_point[3] for time_point in activity_set]\n",
    "        data_row = (overall_set_num, str(activity), activity_set_num, x, y, z)\n",
    "        accelerometer_data.append(data_row)\n",
    "        overall_set_num += 1\n",
    "\n",
    "# Create dataframes directly from the lists of data rows\n",
    "gyroscope_df = ss.createDataFrame(gyroscope_data, schema)\n",
    "accelerometer_df = ss.createDataFrame(accelerometer_data, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'accelerometer_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# subtract 1 from every value in accelerometer_df.overall_set_num\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m accelerometer_df \u001b[39m=\u001b[39m accelerometer_df\u001b[39m.\u001b[39mwithColumn(\u001b[39m\"\u001b[39m\u001b[39moverall_set_num\u001b[39m\u001b[39m\"\u001b[39m, accelerometer_df\u001b[39m.\u001b[39moverall_set_num \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'accelerometer_df' is not defined"
     ]
    }
   ],
   "source": [
    "# subtract 1 from every value in accelerometer_df.overall_set_num\n",
    "accelerometer_df = accelerometer_df.withColumn(\"overall_set_num\", accelerometer_df.overall_set_num - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(accelerometer_df.count() == gyroscope_df.count())\n",
    "assert(accelerometer_df.agg({\"overall_set_num\": \"max\"}).collect()[0][0] == gyroscope_df.agg({\"overall_set_num\": \"max\"}).collect()[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframes to csv\n",
    "gyroscope_df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"bigfiles/gyroscope_data0624.csv\")\n",
    "accelerometer_df.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"bigfiles/accelerometer_data0624.csv\")\n",
    "\n",
    "# load dataframes from csv\n",
    "#gyroscope_df = ss.read.csv(\"bigfiles/gyroscope_data.csv\", header=True, inferSchema=True)\n",
    "#accelerometer_df = ss.read.csv(\"bigfiles/accelerometer_data.csv\", header=True, inferSchema=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accelerometer Measurements chart (only the first result of exercises per activity has been taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to graph\n",
    "for activity in activities_to_process:\n",
    "       t, x, y, z = [], [], [], []\n",
    "\n",
    "       for data_activity in activities_accelerometer_data_dict[activity][0]: # take only the values corresponding to the results of the first excersise\n",
    "              t.append(data_activity[0]) # time value\n",
    "              x.append(data_activity[1]) # X value\n",
    "              y.append(data_activity[2]) # Y value\n",
    "              z.append(data_activity[3]) # Z value\n",
    "\n",
    "       fig, ax = plt.subplots()\n",
    "       ax.plot(t, x, label = 'X')\n",
    "       ax.plot(t, y, label = 'Y')\n",
    "       ax.plot(t, z, label = 'Z')\n",
    "\n",
    "       ax.set(xlabel='Time (seconds)', ylabel='Acceleration output (g)', title=activity)\n",
    "       ax.grid()\n",
    "\n",
    "       fig.tight_layout()\n",
    "       fig.set_size_inches(25, 5)\n",
    "\n",
    "       plt.legend()\n",
    "       plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gyroscope Measurements chart (only the first result of exercises per activity has been taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to graph\n",
    "for activity in activities_to_process:\n",
    "       t, x, y, z = [], [], [], []\n",
    "\n",
    "       for data_activity in activities_gyroscope_data_dict[activity][0]: # take only the values corresponding to the results of the first excersise\n",
    "              t.append(data_activity[0]) # time value\n",
    "              x.append(data_activity[1]) # X value\n",
    "              y.append(data_activity[2]) # Y value\n",
    "              z.append(data_activity[3]) # Z value\n",
    "\n",
    "       fig, ax = plt.subplots()\n",
    "       ax.plot(t, x, label = 'X')\n",
    "       ax.plot(t, y, label = 'Y')\n",
    "       ax.plot(t, z, label = 'Z')\n",
    "\n",
    "       ax.set(xlabel='Time (seconds)', ylabel='Gyroscope output (g)', title=activity)\n",
    "       ax.grid()\n",
    "\n",
    "       fig.tight_layout()\n",
    "       fig.set_size_inches(25, 5)\n",
    "\n",
    "       plt.legend()\n",
    "       plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "for i in range(k):\n",
    "    for col_prefix in ['x', 'y', 'z']:\n",
    "        gyroscope_df = gyroscope_df.withColumn(col_prefix + '_lag_' + str(i), lag(col(col_prefix), i).over(w))\n",
    "        accelerometer_df = accelerometer_df.withColumn(col_prefix + '_lag_' + str(i), lag(col(col_prefix), i).over(w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Pre-Processing\n",
    "Given data points containing x,y,z, and time, smooth this data with a Butterworth low-pass filter (-60dB at 20Hz), then windowed into 5-second windows sliding at 1/50 of a second (i.e., each 5s window shares 4.8s of data with the previous window). Then perform PCA to get the computed features:\n",
    "aX, aYZPC1, and gPC1\n",
    "- aX: the X-axis accelerometer signal\n",
    "- aYZPC1: the projection of only the Y and Z axes onto the first principal component of those two axes. This indicates the movement perpendicular to the user's arm, which allows us to learn about the Y and Z axes regardless of the unknown rotation of the the accelerometer on the armband\n",
    "- gPC1: the projection of the three-dimensional accelerometer signal (x,y,z) onto its first principal component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_set(data_source, source_name, overall_set_num):\n",
    "    window_duration = 5  # Window duration in seconds\n",
    "    sampling_rate = 50  # Sampling rate in Hz\n",
    "    window_size = int(window_duration * sampling_rate) # 250\n",
    "    overlap = 10 # distinct 10 points 200ms, shared 4.8 seconds => 240 overlap\n",
    "\n",
    "    # initialize schemas for accelerometer and gyroscope pyspark dataframes\n",
    "    # accelerometer features: aYZPC and aX\n",
    "    # gyroscope features: gPC\n",
    "    aX_cols = [f\"x{i}\" for i in range(window_size)]\n",
    "    aYZPC1_cols = [f\"aYZPC{i}\" for i in range(window_size)]\n",
    "    gPC1_cols = [f\"gPC{i}\" for i in range(window_size)]\n",
    "\n",
    "    common_fields = [StructField(\"window_index\", IntegerType(), True),\n",
    "        StructField(\"overall_set_num\", IntegerType(), True),\n",
    "        StructField(\"activity_name\", StringType(), True),\n",
    "        StructField(\"activity_set_num\", IntegerType(), True)]\n",
    "\n",
    "    yz_signal_cols = ['y','z']\n",
    "    xyz_signal_cols = ['x'] + yz_signal_cols\n",
    "    yzfields = [StructField(f\"y\", FloatType(), True), \n",
    "        StructField(f\"z\", FloatType(), True)]\n",
    "    \n",
    "    # set the correct schema depending on the source_name\n",
    "    if source_name == \"accelerometer\":\n",
    "        pca_schema = StructType(yzfields)\n",
    "        input_cols = yz_signal_cols\n",
    "        output_schema = StructType(common_fields + [StructField(col, FloatType(), True) for col in aX_cols + aYZPC1_cols])\n",
    "    elif source_name == \"gyroscope\":\n",
    "        pca_schema = StructType(yzfields + \\\n",
    "            [StructField(f\"x\", FloatType(), True)])\n",
    "        input_cols = xyz_signal_cols\n",
    "        output_schema = StructType(common_fields + [StructField(col, FloatType(), True) for col in gPC1_cols])\n",
    "        \n",
    "    # get current set information\n",
    "    set_data = data_source.filter(data_source.overall_set_num == overall_set_num)\n",
    "    activity_name = set_data.select('activity_name').first()[0]\n",
    "    activity_set_num = set_data.select('activity_set_num').first()[0]\n",
    "\n",
    "    # apply butterworth lowpass filter to x, y, z columns\n",
    "    x = list(set_data.select(set_data['x']).toPandas()['x'])\n",
    "    y = list(set_data.select(set_data['y']).toPandas()['y'])\n",
    "    z = list(set_data.select(set_data['z']).toPandas()['z'])\n",
    "    smoothed_x = apply_butterworth_lowpass(x)\n",
    "    smoothed_y = apply_butterworth_lowpass(y)\n",
    "    smoothed_z = apply_butterworth_lowpass(z)\n",
    "\n",
    "    # Slide the window over to compute features for each 5 second interval/window\n",
    "    result_table = []\n",
    "    for i in range(0, len(smoothed_x) - window_size + 1, overlap):\n",
    "        # Get the windowed data that we'll use to perform PCA\n",
    "        individual_window_table = {col: [] for col in input_cols} # initialize empty table\n",
    "        if source_name == \"gyroscope\": # to compute gPC1 with all 3 dimensions (x,y,z)\n",
    "            individual_window_table[\"x\"] = smoothed_x[i:i+window_size]\n",
    "        individual_window_table[\"y\"] = smoothed_y[i:i+window_size]\n",
    "        individual_window_table[\"z\"] = smoothed_z[i:i+window_size]\n",
    "\n",
    "        # Create dict individual_window_table to pandas DF\n",
    "        individual_window_table = pd.DataFrame.from_dict(individual_window_table)\n",
    "        # Convert the pandas DataFrame to a PySpark DataFrame\n",
    "        pyspark_window_df = ss.createDataFrame(individual_window_table, pca_schema)\n",
    "\n",
    "        # Combine the input columns into a single vector column\n",
    "        assembler = VectorAssembler(inputCols=input_cols, outputCol='features_vect')\n",
    "        assembled_df = assembler.transform(pyspark_window_df)\n",
    "\n",
    "        # Perform PCA and keep only the first principal component\n",
    "        pca = PCA(k=1, inputCol='features_vect', outputCol='pca_features')\n",
    "        pca_model = pca.fit(assembled_df)\n",
    "        pca_result = pca_model.transform(assembled_df).select('pca_features')\n",
    "\n",
    "        # Flatten the pca_result DataFrame so that each element is a floatType and then transpose it\n",
    "        pca_result = pca_result.rdd.map(lambda x: [float(element) for element in x.pca_features.toArray()]) \\\n",
    "            .flatMap(lambda x: x).collect()\n",
    "\n",
    "        # Append the pca_row and other columns to the result_table\n",
    "        # TODO: maybe using smoothed_x would be better than raw x, try this to see if it improves performance\n",
    "        if source_name == \"accelerometer\":\n",
    "            result_table.append([i, overall_set_num, activity_name, activity_set_num] + x[i:i+window_size] + pca_result)\n",
    "        elif source_name == \"gyroscope\":\n",
    "            result_table.append([i, overall_set_num, activity_name, activity_set_num] + pca_result)\n",
    "\n",
    "    # Write result_table to a file\n",
    "    filename = f'bigfiles/{source_name}/result_table_{overall_set_num}_0625.csv'\n",
    "    with open(filename, 'w') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerows(result_table)\n",
    "\n",
    "    return filename\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data_source, source_name):\n",
    "    # Iterate over all distinct set_nums in the data table\n",
    "    # So that we can process 1 exercise set's signal in all 4 dimensions (x, y, z, t) at a time\n",
    "    # Use all of the available CPU cores to preprocess all of the sets in parallel\n",
    "    overall_set_nums = data_source.select('overall_set_num').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "    num_processes = multiprocessing.cpu_count()  \n",
    "    pool = multiprocessing.Pool(processes=num_processes)\n",
    "    output_filenames = pool.map(process_set, overall_set_nums)\n",
    "    return output_filenames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join both dataframes\n",
    "(TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join gyroscope_features and accelerometer_features by overall_set_num, window_index, activity_name\n",
    "joined_sparkdf = gyroscope_features.join(\n",
    "    accelerometer_features,\n",
    "    (gyroscope_features[\"overall_set_num\"] == accelerometer_features[\"overall_set_num\"]) &\n",
    "    (gyroscope_features[\"window_index\"] == accelerometer_features[\"window_index\"]) &\n",
    "    (gyroscope_features[\"activity_name\"] == accelerometer_features[\"activity_name\"]),\n",
    "    \"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_sparkdf.count()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognition features (computed for each signal)\n",
    "20 features computed over each of 3 derived signals (aX, aYZPC1, and gPC1)\n",
    "- Autocorrelation bins: 5 evenly-spaced bins of the 5- second autocorrelation â€“ summed per bin (5 features).\n",
    "- RMS: The root-mean-square amplitude of the signal.\n",
    "- Power bands: The magnitude of the power spectrum in 10 bands spaced linearly from 0.1-25Hz (10 features).\n",
    "- Mean, standard deviation, kurtosis, interquartile range (4 features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get_variance = udf(lambda row: np.variance([x for x in row]), IntegerType())\n",
    "\n",
    "new_df = df.withColumn(\"null_count\", count_empty_columns(struct([df[x] for x in df.columns])))\n",
    "\n",
    "new_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change to use column prefix\n",
    "\n",
    "def get_autocorrelation_bins(set_df, col):\n",
    "    # Compute the autocorrelation for current column (col)\n",
    "    window = Window.orderBy(\"signal\")\n",
    "    set_df = set_df.withColumn(\"lag\", lag(col(\"signal\")).over(window))\n",
    "    lags = np.array(set_df.select(\"lag\").rdd.flatMap(lambda x: x).collect())\n",
    "    signals = np.array(set_df.select(\"signal\").rdd.flatMap(lambda x: x).collect())\n",
    "    autocorrelation = np.correlate(signals, lags, mode=\"full\")\n",
    "\n",
    "    # Bin autocorrelation values\n",
    "    num_bins = 5\n",
    "    ac_length = len(autocorrelation)\n",
    "\n",
    "    # Compute the lag interval between consecutive bins\n",
    "    lag_interval = ac_length / num_bins\n",
    "\n",
    "    # Create an array to store the sums for each bin\n",
    "    bin_sums = np.zeros(num_bins)\n",
    "\n",
    "    # Iterate over the autocorrelation values and add to the corresponding bin\n",
    "    for i, value in enumerate(autocorrelation):\n",
    "        bin_index = int(i // lag_interval)\n",
    "        if bin_index < num_bins:\n",
    "            bin_sums[bin_index] += value\n",
    "\n",
    "    return bin_sums\n",
    "\n",
    "def get_rms(set_df, col):\n",
    "    # Calculate the RMS amplitude\n",
    "    rms_df = set_df.select(sqrt(avg(abs(col(\"signal\").cast(\"double\")) ** 2)).alias(\"rms_amplitude\"))\n",
    "    return rms_df.first()[\"rms_amplitude\"]\n",
    "\n",
    "def get_mean(set_df, col):\n",
    "    # Calculate the mean\n",
    "    mean_df = set_df.select(avg(col(\"signal\")).alias(\"mean\"))\n",
    "    return mean_df.first()[\"mean\"]\n",
    "\n",
    "def get_stddev(set_df, col):\n",
    "    # Calculate the standard deviation\n",
    "    stddev_df = set_df.select(stddev(col(\"signal\")).alias(\"stddev\"))\n",
    "    return stddev_df.first()[\"stddev\"]\n",
    "\n",
    "def get_stddev(row):\n",
    "    data = [float(x.strip()) for x in row.split(\",\")]\n",
    "    return np.variance(data)\n",
    "\n",
    "def get_kurtosis(row):\n",
    "    \n",
    "    return scipy.stats.kurtosis\n",
    "\n",
    "def row_to_list\n",
    "\n",
    "def get_interquartile_range(set_df, col):\n",
    "    # Calculate the interquartile range\n",
    "    quartiles = set_df.approxQuantile(col, [0.25, 0.75], 0.0)\n",
    "    iqr = quartiles[1] - quartiles[0]\n",
    "    return iqr\n",
    "\n",
    "def get_powerbands(set_df, col):\n",
    "    # Define the frequency bands\n",
    "    lower_bounds = [0.1, 2.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5, 20.0, 22.5]\n",
    "    upper_bounds = [2.5, 5.0, 7.5, 10.0, 12.5, 15.0, 17.5, 20.0, 22.5, 25.0]\n",
    "\n",
    "    # Convert frequency column to vector column\n",
    "    vectorAssembler = VectorAssembler(inputCols=[\"frequency\"], outputCol=\"features\")\n",
    "    set_df = vectorAssembler.transform(set_df)\n",
    "\n",
    "    # Define a user-defined function (UDF) to compute the power spectrum\n",
    "    def compute_power_spectrum(features):\n",
    "        power_spectrum = [value * value for value in features]\n",
    "        return Vectors.dense(power_spectrum)\n",
    "\n",
    "    compute_power_spectrum_udf = udf(compute_power_spectrum, DoubleType())\n",
    "\n",
    "    # Compute the power spectrum and create a new column\n",
    "    df = df.withColumn(\"power_spectrum\", compute_power_spectrum_udf(\"features\"))\n",
    "\n",
    "    # Define bucketizer splits based on the frequency bands\n",
    "    bucketizer = Bucketizer(splits=[lower_bounds, upper_bounds], inputCol=\"frequency\", outputCol=\"bucket\")\n",
    "\n",
    "    # Apply bucketizer and compute the sum of power spectrum within each band\n",
    "    df = bucketizer.transform(df)\n",
    "    result = df.groupby(\"bucket\").agg({\"power_spectrum\": \"sum\"}).orderBy(\"bucket\")\n",
    "\n",
    "    # Show the computed power bands\n",
    "    result.show()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_signal_features(window, signal_prefix):\n",
    "    features = []\n",
    "    features.extend(get_autocorrelation_bins(window, signal_prefix)) # 5 features\n",
    "    features.append(get_rms(window, signal_prefix)) # 1 feature\n",
    "    features.append(get_mean(window, signal_prefix)) # 1 feature\n",
    "    features.append(get_stddev(window, signal_prefix)) # 1 feature\n",
    "    features.append(get_kurtosis(window, signal_prefix)) # 1 feature\n",
    "    features.extend(get_interquartile_range(window, signal_prefix)) # 2 features\n",
    "    features.extend(get_powerbands(window, signal_prefix)) # 10 features\n",
    "    return features\n",
    "\n",
    "def get_signal_computed_feature_names(signal_prefix):\n",
    "    names = [f\"{signal_prefix}_autocorrelation_bin{ab}\" for ab in range(5)]\n",
    "    names.append(f\"{signal_prefix}_rms_amplitude\")\n",
    "    names.append(f\"{signal_prefix}_mean\")\n",
    "    names.append(f\"{signal_prefix}_stddev\")\n",
    "    names.append(f\"{signal_prefix}_kurtosis\")\n",
    "    names.append(f\"{signal_prefix}_interquartile_range_start\")\n",
    "    names.append(f\"{signal_prefix}_interquartile_range_end\")\n",
    "    names.extend([f\"{signal_prefix}_powerband{pb}\" for pb in range(10)])\n",
    "    return names\n",
    "\n",
    "def get_signal_schema(signal_prefix):\n",
    "    return [StructField(name, FloatType(), True)\n",
    "        for name in get_signal_computed_feature_names(signal_prefix)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 20 signal features for each signal for every 5 second window (row)\n",
    "derived_signals = ['aYZPC1', 'aX', 'gPC1']\n",
    "for signal_prefix in derived_signals:\n",
    "    signal_columns = [f\"{signal_prefix}{i}\" for i in range(250)]\n",
    "    schema = get_signal_computed_feature_names(signal_prefix)\n",
    "    udf_get_signal_features = udf(get_signal_features, schema) # Register the UDF\n",
    "    for i, col_name in enumerate(get_signal_computed_feature_names(signal_prefix)):\n",
    "        joined_sparkdf = joined_sparkdf.withColumn(col_name, udf_get_signal_features(*signal_columns)[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca4bdd2ec3710d32cba740d0f2a8504a4cffd42a76bfba64146e079be5d25276"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

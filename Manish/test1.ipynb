{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%% Constants and environment\n",
    "\n",
    "%#ok<*NBRAK>\n",
    "dataBaseDir = 'D:\\temp\\exercise_data_release';\n",
    "\n",
    "% This file has *only* data during exercises, already separated out by exercise.  \n",
    "% The \"singleonly\" in the filename means that it only contains \"single-activity\" traces.\n",
    "% So this is useful for the \"which exercise?\" (recognition) and counting problems, not for \n",
    "% the \"when is there exercise?\" problem.\n",
    "dataFileSingleActivity = fullfile(dataBaseDir,'exercise_data.50.0000_singleonly.mat');\n",
    "% This file has data from complete exercise sessions, so it includes lots of time\n",
    "% where there's no execise happening.  The \"multionly\" in the filename means that it \n",
    "% only contains \"multi-activity\" traces.  So this is useful for the \"when are you \n",
    "% exercising?\"  problem (segmentation).\n",
    "dataFileMultiActivity = fullfile(dataBaseDir,'exercise_data.50.0000_multionly.mat');\n",
    "\n",
    "% We separate these into two files to make it a little easier to work on\n",
    "% one problem or the other.  You can completely reconstruct the\n",
    "% \"single-activity\" file *from* the \"multi-activity\" file, because it also\n",
    "% includes all the times at which each exercise started and stopped.\n",
    "\n",
    "% In this file, we're going to play around with data from a particular \n",
    "% subject doing a particular exercise.  Not every subject did every\n",
    "% exercise; I'm picking a combination that I happen to know isn't empty.\n",
    "% In fact I'm also picking a subject that I happen to know participated \n",
    "% more than once.\n",
    "iSubject = 52;\n",
    "exerciseName = 'Two-arm Dumbbell Curl (both arms, not alternating)';\n",
    "\n",
    "\n",
    "%% Load data... this will take a couple minutes; it's about 2.5GB\n",
    "\n",
    "fprintf(1,'Loading single-activity data...\\n');\n",
    "exerciseDataSingleActivity = load(dataFileSingleActivity);\n",
    "\n",
    "fprintf(1,'Loading multi-activity data...\\n');\n",
    "exerciseDataMultiActivity = load(dataFileMultiActivity);\n",
    "\n",
    "fprintf(1,'Finished loading data\\n');\n",
    "\n",
    "% Both of these files have similar formats format... the data lives in a cell\n",
    "% matrix called \"subject_data\", where each row is a subject, and each column\n",
    "% is a type of exercise.  \n",
    "%\n",
    "% In the \"multi-activity\" data, there's only one column, since we haven't separated\n",
    "% data out into exercises yet.\n",
    "\n",
    "% The column names for the single-activity data are here:\n",
    "activities = exerciseDataSingleActivity.exerciseConstants.activities';\n",
    "nActivityTypes = length(activities);\n",
    "\n",
    "% Sanity-check our data matrices\n",
    "assert(nActivityTypes == size(exerciseDataSingleActivity.subject_data,2));\n",
    "\n",
    "assert(1 == size(exerciseDataMultiActivity.subject_data,2));\n",
    "\n",
    "% Both files should have the same number of subjects\n",
    "nParticipants = size(exerciseDataSingleActivity.subject_data,1);\n",
    "assert(nParticipants == size(exerciseDataMultiActivity.subject_data,1));\n",
    "\n",
    "\n",
    "%% Find the column for a particular exercise we want to look at\n",
    "\n",
    "exerciseIndex = find(strcmp(exerciseDataSingleActivity.exerciseConstants.activities,...\n",
    "    exerciseName));\n",
    "\n",
    "\n",
    "%% This matrix has one row per person, one column per exercise\n",
    "\n",
    "% Any given cell in \"subject_data\" is a struct array, with one element for\n",
    "% each time that subject (this row) performed that exercise (this column)\n",
    "%\n",
    "% So for example, the following cell contains a struct array, with one element per \n",
    "% record, of every time this subject came in to our lab and did the exercise we picked \n",
    "% above, which may span multiple visits.\n",
    "recordings = exerciseDataSingleActivity.subject_data{iSubject,exerciseIndex};\n",
    "\n",
    "% How many times did this subject do this particular exercise?\n",
    "nRecordings = length(recordings);\n",
    "\n",
    "\n",
    "%% Now let's look at one instance of this particular subject doing this particular exercise\n",
    "\n",
    "% Arbitrarily grab the first instance to plot\n",
    "recording = recordings(1);\n",
    "assert(strcmp(recording.activityName,exerciseName));\n",
    "\n",
    "% Plot the raw accelerometer and gyro data (at 50Hz)\n",
    "accelT = recording.data.accelDataMatrix(:,1);\n",
    "accelXYZ = recording.data.accelDataMatrix(:,[2:4]);\n",
    "gyroT = recording.data.gyroDataMatrix(:,1);\n",
    "gyroXYZ = recording.data.gyroDataMatrix(:,[2:4]);\n",
    "\n",
    "subplot(2,1,1);\n",
    "plot(accelT,accelXYZ);\n",
    "xlabel('Time (seconds)');\n",
    "ylabel('Accelerometer output (g)');\n",
    "legend({'X','Y','Z'});\n",
    "\n",
    "subplot(2,1,2);\n",
    "plot(gyroT,gyroXYZ);\n",
    "xlabel('Time (seconds)');\n",
    "ylabel('Gyro output (dps)');\n",
    "legend({'X','Y','Z'});\n",
    "\n",
    "\n",
    "%% Now let's play around with the multi-activity data\n",
    "\n",
    "% Remember, in this file, we haven't separated out the periods of exercise\n",
    "% and non-exercise, everything is one big long trace per subject, with\n",
    "% labels to tell us where exercises started and stopped.\n",
    "\n",
    "recordings = exerciseDataMultiActivity.subject_data{iSubject,1};\n",
    "\n",
    "% Each instance here represents a visit to our lab\n",
    "nVisits = length(recordings);\n",
    "\n",
    "% Arbitrarily grab the first visit\n",
    "recording = recordings(1);\n",
    "\n",
    "% This is a cell matrix that tells us when exercises started and stopped,\n",
    "% and how many repetitions the subject did for each exercise.  The columns \n",
    "% are:\n",
    "%\n",
    "% [exercise name],[start time],[end time],[notes],[number of repetitions]\n",
    "\n",
    "% So let's plot this subject's accelerometer data (just one axis, so the plot\n",
    "% doesn't get too complex), with vertical lines to indicate where exercises\n",
    "% started, with labels for each exercise.\n",
    "nActivities = size(recording.activityStartMatrix,1);\n",
    "\n",
    "accelT = recording.data.accelDataMatrix(:,1);\n",
    "accelZ = recording.data.accelDataMatrix(:,4);\n",
    "\n",
    "plot(accelT,accelZ);\n",
    "hold on;\n",
    "xlabel('Time (seconds)');\n",
    "ylabel('Accelerometer output (g)');\n",
    "\n",
    "for(iActivity=1:nActivities)\n",
    "    activityName = recording.activityStartMatrix{iActivity,1};\n",
    "    if (strcmpi(activityName,'non-exercise'))\n",
    "        continue;\n",
    "    end\n",
    "    activityCount = recording.activityStartMatrix{iActivity,5};\n",
    "    activityStartTime = recording.activityStartMatrix{iActivity,2};\n",
    "    activityEndTime = recording.activityStartMatrix{iActivity,3};\n",
    "    lineHandle = line([activityStartTime activityStartTime],[-0.5 0.5]);\n",
    "    lineHandle.Color = [0 1 0];\n",
    "    lineHandle = line([activityEndTime activityEndTime],[-0.5 0.5]);\n",
    "    lineHandle.Color = [1 0 0];\n",
    "    yValue = -0.5 + rand();\n",
    "    tHandle = text(activityStartTime,yValue,sprintf('%s x %d',activityName,activityCount));    \n",
    "    tHandle.Rotation = 45;\n",
    "end % ...for each activity\n",
    "\n",
    "hold off; zoom on;\n",
    "xlim([100 300])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy.signal as signal\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "- what is \"<Initial Activity>\" as a an activity name?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load & Explore Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and save the 'exercise_data.50.0000_singleonly.mat' file from the below link and save it locally in the same folder as this notebook file. \n",
    "\n",
    "https://msropendata.com/datasets/799c1167-2c8f-44c4-929c-227bf04e2b9a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load exercise dataset \n",
    "exercise_dataset = scipy.io.loadmat('/Users/mani/Downloads/P/exerciserecognitionfromwearablesensors/exercise_data.50.0000_singleonly.mat', struct_as_record=False)\n",
    "\n",
    "# Load activities and data full objects\n",
    "exercise_constants = exercise_dataset['exerciseConstants'][0][0].activities\n",
    "subject_data = exercise_dataset['subject_data']\n",
    "\n",
    "# extract activities names into an array\n",
    "all_activities = []\n",
    "for act in exercise_constants[0]:\n",
    "    all_activities.append(act[0])\n",
    "    #print (act[0]) # print values for reference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_activities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only take data from 3 random exercises\n",
    "activities_to_process = all_activities\n",
    "print(activities_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define dictionaries for accelerometer and gyroscope data\n",
    "# activities_accelerometer_data_dict = { activities_to_process[0]: [], activities_to_process[1]: [], activities_to_process[2]: []}\n",
    "\n",
    "\n",
    "\n",
    "activities_gyroscope_data_dict = { activities_to_process[0]: [], activities_to_process[1]: [], activities_to_process[2]: []}\n",
    "rep_counts_actual = []\n",
    "\n",
    "# iterate over subject data to search for those activities and save the data related\n",
    "for data_item in subject_data:\n",
    "    for x in data_item:\n",
    "        if len(x) > 0:\n",
    "            if x[0] is not None and len(x[0]) > 0:\n",
    "                data_activity_name = x[0,0].activityName[0]\n",
    "                data_activity_reps = x[0,0].activityReps[0]\n",
    "                data_item_accelDataMatrix = x[0,0].data[0,0].accelDataMatrix\n",
    "                data_item_gyroDataMatrix = x[0,0].data[0,0].gyroDataMatrix\n",
    "                if data_activity_name in activities_to_process:\n",
    "                    activities_accelerometer_data_dict[data_activity_name].append(data_item_accelDataMatrix)\n",
    "                    activities_gyroscope_data_dict[data_activity_name].append(data_item_gyroDataMatrix)\n",
    "                    rep_counts_actual.append(data_activity_reps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_accelDataMatrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_item_gyroDataMatrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deeper look at files\n",
    "Below cells are just to see what fields are in the matlab files. Turning them into dictionaries gives a better view of the mat_struct objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in exercise_dataset:\n",
    "    print (x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(exercise_dataset['exerciseConstants'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadmat(filename):\n",
    "    '''\n",
    "    this function should be called instead of direct scipy.io.loadmat\n",
    "    as it cures the problem of not properly recovering python dictionaries\n",
    "    from mat files. It calls the function check keys to cure all entries\n",
    "    which are still mat-objects\n",
    "    '''\n",
    "    data = scipy.io.loadmat(filename, struct_as_record=False, squeeze_me=True)\n",
    "    return _check_keys(data)\n",
    "\n",
    "def _check_keys(dict):\n",
    "    '''\n",
    "    checks if entries in dictionary are mat-objects. If yes\n",
    "    todict is called to change them to nested dictionaries\n",
    "    '''\n",
    "    for key in dict:\n",
    "        if isinstance(dict[key], scipy.io.matlab.mio5_params.mat_struct):\n",
    "            dict[key] = _todict(dict[key])\n",
    "    return dict        \n",
    "\n",
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, scipy.io.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "exercise_dataset2 = loadmat('/Users/mani/Downloads/P/exerciserecognitionfromwearablesensors/exercise_data.50.0000_singleonly.mat')\n",
    "exercise_dataset2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_dataset2['exerciseConstants'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_dataset2['subject_data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _todict(matobj):\n",
    "    '''\n",
    "    A recursive function which constructs from matobjects nested dictionaries\n",
    "    '''\n",
    "    dict = {}\n",
    "    for strg in matobj._fieldnames:\n",
    "        elem = matobj.__dict__[strg]\n",
    "        if isinstance(elem, scipy.io.matlab.mio5_params.mat_struct):\n",
    "            dict[strg] = _todict(elem)\n",
    "        else:\n",
    "            dict[strg] = elem\n",
    "    return dict\n",
    "\n",
    "_todict(exercise_dataset2['subject_data'][0][3][0])\n",
    "\n",
    "# TODO: what is boundingWindow?\n",
    "# activityReps = ground truth for reps\n",
    "# activityName = ground truth for name of activity/exercise\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(all_activities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activities_accelerometer_data_dict[activities_to_process[0]][1].shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we see that each recorded activity set has a varying duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activity in activities_to_process:\n",
    "    print(\"Activity: \" + activity)\n",
    "    print(\"Accelerometer data shape: \" + str(len(activities_accelerometer_data_dict[activity])))\n",
    "    print(\"Gyroscope data shape: \" + str(len(activities_gyroscope_data_dict[activity])))\n",
    "    print(\"\")\n",
    "    for i in activities_accelerometer_data_dict[activity]:\n",
    "        print(activity + \"accelerometer data_ray_shape: \" + str(i.shape))\n",
    "    for i in activities_accelerometer_data_dict[activity]:\n",
    "        print(activity + \"gyroscope data_ray_shape: \" + str(i.shape))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accelerometer Measurements chart (only the first result of exercises per activity has been taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to graph\n",
    "for activity in activities_to_process:\n",
    "       t, x, y, z = [], [], [], []\n",
    "\n",
    "       for data_activity in activities_accelerometer_data_dict[activity][0]: # take only the values corresponding to the results of the first excersise\n",
    "              t.append(data_activity[0]) # time value\n",
    "              x.append(data_activity[1]) # X value\n",
    "              y.append(data_activity[2]) # Y value\n",
    "              z.append(data_activity[3]) # Z value\n",
    "\n",
    "       fig, ax = plt.subplots()\n",
    "       ax.plot(t, x, label = 'X')\n",
    "       ax.plot(t, y, label = 'Y')\n",
    "       ax.plot(t, z, label = 'Z')\n",
    "\n",
    "       ax.set(xlabel='Time (seconds)', ylabel='Acceleration output (g)', title=activity)\n",
    "       ax.grid()\n",
    "\n",
    "       fig.tight_layout()\n",
    "       fig.set_size_inches(25, 5)\n",
    "\n",
    "       plt.legend()\n",
    "       plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gyroscope Measurements chart (only the first result of exercises per activity has been taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to graph\n",
    "for activity in activities_to_process:\n",
    "       t, x, y, z = [], [], [], []\n",
    "\n",
    "       for data_activity in activities_gyroscope_data_dict[activity][0]: # take only the values corresponding to the results of the first excersise\n",
    "              t.append(data_activity[0]) # time value\n",
    "              x.append(data_activity[1]) # X value\n",
    "              y.append(data_activity[2]) # Y value\n",
    "              z.append(data_activity[3]) # Z value\n",
    "\n",
    "       fig, ax = plt.subplots()\n",
    "       ax.plot(t, x, label = 'X')\n",
    "       ax.plot(t, y, label = 'Y')\n",
    "       ax.plot(t, z, label = 'Z')\n",
    "\n",
    "       ax.set(xlabel='Time (seconds)', ylabel='Gyroscope output (g)', title=activity)\n",
    "       ax.grid()\n",
    "\n",
    "       fig.tight_layout()\n",
    "       fig.set_size_inches(25, 5)\n",
    "\n",
    "       plt.legend()\n",
    "       plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Pre-Processing\n",
    "Given data points containing x,y,z, and time, how would you smooth this data with a Butterworth low-pass filter (-60dB at 20Hz), then windowed into 5-second windows sliding at 200ms (i.e., each 5s window shares 4.8s of data with the previous window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_butterworth_lowpass(data, sampling_rate, cutoff_frequency, filter_order):\n",
    "    nyquist_frequency = 0.5 * sampling_rate  # Updated line\n",
    "    normalized_cutoff_frequency = cutoff_frequency / nyquist_frequency\n",
    "    b, a = signal.butter(filter_order, normalized_cutoff_frequency, btype='low', analog=False, output='ba')\n",
    "    smoothed_data = signal.lfilter(b, a, data)\n",
    "    return smoothed_data\n",
    "\n",
    "# Initialize a dictionary windowed_smoothed_data with keys as activities_to_process and corresponding values as empty lists\n",
    "windowed_smoothed_data = {activity: [] for activity in activities_to_process}\n",
    "\n",
    "# Assuming 'x', 'y', 'z', and 'time' are your data arrays in seconds\n",
    "N = 4  # Filter order\n",
    "sampling_rate = 1.0  # Sampling rate of 1 Hz\n",
    "cutoff_frequency = 0.2  # Cutoff frequency of 0.2 Hz\n",
    "filter_order = N  # Specify the desired filter order\n",
    "\n",
    "# create embeddings\n",
    "def preprocess(data_source, source_name):\n",
    "    window_size = 250 # Window size of 5 seconds = 250 points\n",
    "    cols = ['x', 'y', 'z', 't']\n",
    "    cols_expanded = ([col + str(i) for col in cols for i in range(window_size)])\n",
    "    windowed_smoothed_data_table = {col: [] for col in cols_expanded}\n",
    "    windowed_smoothed_data_table['activity'] = []\n",
    "    windowed_smoothed_data_table['set_num'] = []\n",
    "\n",
    "    for activity in activities_to_process:\n",
    "        for set_num, data_activity_set in enumerate(data_source[activity]):\n",
    "            # Create empty arrays to store the windowed data\n",
    "            windowed_x = []\n",
    "            windowed_y = []\n",
    "            windowed_z = []\n",
    "            windowed_t = []\n",
    "\n",
    "            t = data_activity_set[:,0]\n",
    "            x = data_activity_set[:,1]\n",
    "            y = data_activity_set[:,2]\n",
    "            z = data_activity_set[:,3]\n",
    "\n",
    "            # Normalize the data arrays to be less than abs(1)\n",
    "            max_value = max(max(x), max(y), max(z), max(t))\n",
    "            x_normalized = [value / max_value for value in x]\n",
    "            y_normalized = [value / max_value for value in y]\n",
    "            z_normalized = [value / max_value for value in z]\n",
    "\n",
    "            # Apply the Butterworth filter (reduces noise)\n",
    "            smoothed_x = apply_butterworth_lowpass(x_normalized, sampling_rate, cutoff_frequency, filter_order)\n",
    "            smoothed_y = apply_butterworth_lowpass(y_normalized, sampling_rate, cutoff_frequency, filter_order)\n",
    "            smoothed_z = apply_butterworth_lowpass(z_normalized, sampling_rate, cutoff_frequency, filter_order)\n",
    "\n",
    "            # Slide the window over the smoothed data \n",
    "            overlap = 1 # Overlap of 4.8 seconds\n",
    "            windowed_x = np.array([smoothed_x[i:i+window_size] for i in range(0, len(smoothed_x) - window_size + 1, overlap)])\n",
    "            windowed_y = np.array([smoothed_y[i:i+window_size] for i in range(0, len(smoothed_y) - window_size + 1, overlap)])\n",
    "            windowed_z = np.array([smoothed_z[i:i+window_size] for i in range(0, len(smoothed_z) - window_size + 1, overlap)])\n",
    "            windowed_t = np.array([t[i:i+window_size] for i in range(0, len(t) - window_size + 1, overlap)])\n",
    "\n",
    "            # The windowed_x, windowed_y, and windowed_z arrays now contain the windowed data\n",
    "            window_row_count = windowed_x.shape[0]\n",
    "            windowed_smoothed_data_table['activity'].extend([activity] * window_row_count)\n",
    "            windowed_smoothed_data_table['set_num'].extend([set_num] * window_row_count)\n",
    "            windowed_vals = [windowed_x, windowed_y, windowed_z, windowed_t]\n",
    "            for c, col in enumerate(cols):\n",
    "                for i in range(window_size):\n",
    "                    windowed_smoothed_data_table[col+str(i)].extend(windowed_vals[c][:,i])\n",
    "                    \n",
    "    \n",
    "    assert(len(windowed_smoothed_data_table['x0']) == len(windowed_smoothed_data_table['y0']) == len(windowed_smoothed_data_table['z0']) == len(windowed_smoothed_data_table['t0']))\n",
    "    df = pd.DataFrame(windowed_smoothed_data_table)\n",
    "    df['source'] = source_name\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gyroscope = preprocess(activities_gyroscope_data_dict, 'gyroscope')\n",
    "df_accelerometer = preprocess(activities_accelerometer_data_dict, 'accelerometer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(len([col for col in df_gyroscope.columns if col.startswith('x')]) \n",
    "    == len([col for col in df_gyroscope.columns if col.startswith('y')]) \n",
    "    == len([col for col in df_gyroscope.columns if col.startswith('z')]) \n",
    "    == len([col for col in df_gyroscope.columns if col.startswith('t')]))\n",
    "print(\"number of time points in each window \", len([col for col in df_gyroscope.columns if col.startswith('t')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(df_accelerometer['set_num'].max() == (len(df_gyroscope['set_num'].unique())-1))\n",
    "# add a column called rep_counts_actual to df_accelerometer where the value for that set_num corresponds to the index in rep_counts_actual \n",
    "df_accelerometer['rep_counts_actual'] = df_accelerometer['set_num'].apply(lambda x: rep_counts_actual[x])\n",
    "df_gyroscope['rep_counts_actual'] = df_gyroscope['set_num'].apply(lambda x: rep_counts_actual[x])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gyroscope.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the source, set_num, and activity columns as the index\n",
    "df_gyroscope.set_index(['source', 'set_num', 'activity'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Feature Computation\n",
    "1) aX: the X-axis accelerometer signal\n",
    "\n",
    "2) aXmag: the magnitude of the accelerometer signal at each sample, i.e. sqrt(ax2+ay2+az2).\n",
    "\n",
    "3) aPC1: the projection of the three-dimensional accelerometer signal onto its first principal component. This is the movement along the axis that demonstrates the most variance within this window, or – anecdotally – themost “interesting” rotation of the window.\n",
    "\n",
    "4) aYZPC1: the projection of only the Y and Z axes onto the first principal component of those two axes. This captures movement perpendicular to the arm, which allows us to derive information from the Y and Z axes despite the unknown rotation of the armband. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pc1(df):\n",
    "    # perform PCA on each row of df with all columns that start with 'x' or 'y' or 'z'\n",
    "    dim_cols = [col for col in df.columns if col.startswith('x') or col.startswith('y') or col.startswith('z') or col.startswith('t')]\n",
    "    window = df[dim_cols].iloc[0]\n",
    "    pc1 = []\n",
    "\n",
    "    for w in range(len(df)):\n",
    "        #for w in range(10):\n",
    "        window = df[dim_cols].iloc[w]\n",
    "        window_df = {}\n",
    "        ts = list(window[[col for col in window.index if col.startswith('t')]].values)\n",
    "        xs = list(window[[col for col in window.index if col.startswith('x')]].values)\n",
    "        ys = list(window[[col for col in window.index if col.startswith('y')]].values)\n",
    "        zs = list(window[[col for col in window.index if col.startswith('z')]].values)\n",
    "\n",
    "        window_df = {'t': ts, 'x': xs, 'y': ys, 'z': zs}\n",
    "        window_df = pd.DataFrame(window_df)\n",
    "        window_df.set_index('t', inplace=True)\n",
    "\n",
    "        # Perform PCA on the data\n",
    "        pca = PCA(n_components=1) # only keep the first principal component\n",
    "        principal_components = pca.fit_transform(window_df)\n",
    "        pc1.append(principal_components[:, 0])\n",
    "    assert(len(pc1) == len(df))\n",
    "    return pc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def get_pc1(df):\n",
    "    dim_cols = [col for col in df.columns if col.startswith('x') or col.startswith('y') or col.startswith('z') or col.startswith('t')]\n",
    "    window = df[dim_cols].iloc[0]\n",
    "\n",
    "    ts_cols = [col for col in window.index if col.startswith('t')]\n",
    "    xyz_cols = [col for col in window.index if col.startswith('x') or col.startswith('y') or col.startswith('z')]\n",
    "\n",
    "    window_df = pd.DataFrame(index=window[ts_cols].values, columns=['x', 'y', 'z'])\n",
    "\n",
    "    pc1 = []\n",
    "\n",
    "    for _, row in df[dim_cols].iterrows():\n",
    "        window_df['x'] = row[xyz_cols[:len(ts_cols)]].values\n",
    "        window_df['y'] = row[xyz_cols[len(ts_cols):2*len(ts_cols)]].values\n",
    "        window_df['z'] = row[xyz_cols[2*len(ts_cols):]].values\n",
    "\n",
    "        pca = PCA(n_components=1)\n",
    "        principal_components = pca.fit_transform(window_df.values)\n",
    "        pc1.append(principal_components[0, 0])\n",
    "\n",
    "    assert len(pc1) == len(df)\n",
    "    return pc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate percentage of nan in each column of df_gyroscope where percentage is greater than 0\n",
    "nan_percentage = df_gyroscope.isna().sum() / len(df_gyroscope)\n",
    "nan_percentage[nan_percentage > 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def calculate_pca(window):\n",
    "    window_df = {}\n",
    "    ts = list(window[[col for col in window.index if col.startswith('t')]].values)\n",
    "    xs = list(window[[col for col in window.index if col.startswith('x')]].values)\n",
    "    ys = list(window[[col for col in window.index if col.startswith('y')]].values)\n",
    "    zs = list(window[[col for col in window.index if col.startswith('z')]].values)\n",
    "\n",
    "    window_df = {'t': ts, 'x': xs, 'y': ys, 'z': zs}\n",
    "    window_df = pd.DataFrame(window_df)\n",
    "    window_df.set_index('t', inplace=True)\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    principal_components = pca.fit_transform(window_df)\n",
    "    return principal_components\n",
    "\n",
    "def get_pc1(df):\n",
    "    dim_cols = [col for col in df.columns if col.startswith('x') or col.startswith('y') or col.startswith('z') or col.startswith('t')]\n",
    "    pc1 = Parallel(n_jobs=-1)(delayed(calculate_pca)(row[dim_cols]) for w, row in df.iterrows())\n",
    "    assert len(pc1) == len(df)\n",
    "    return pc1\n",
    "\n",
    "def add_pc_cols(df, pc, pc_name):\n",
    "    # create columns for every index of pc[0] in df \n",
    "    for i in range(len(pc[0])):\n",
    "        df[pc_name+'_'+str(i)] = [pc[j][i] for j in range(len(pc))]\n",
    "    assert(len([col for col in df.columns if col.startswith(pc_name)]) == len(pc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = get_pc1(df_gyroscope)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pc1[0]))\n",
    "print(len(df_gyroscope))\n",
    "print(len(pc1))\n",
    "\n",
    "add_pc_cols(df_gyroscope, pc1, 'pc1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc1 = get_pc1(df_accelerometer)\n",
    "print(len(pc1[0]))\n",
    "print(len(df_accelerometer))\n",
    "print(len(pc1))\n",
    "\n",
    "add_pc_cols(df_accelerometer, pc1, 'pc1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to do delete this version of the function as it's too slow\n",
    "def get_pc2(df):\n",
    "    # perform PCA on df.iloc[0] columns that start with 'x' or 'y' or 'z'\n",
    "    dim_cols = [col for col in df.columns if col.startswith('x') or col.startswith('y') or col.startswith('z') or col.startswith('t')]\n",
    "    pc2 = []\n",
    "    for w in range(len(df)):\n",
    "        window = df[dim_cols].iloc[w]\n",
    "        window_df = {}\n",
    "        ts = list(window[[col for col in window.index if col.startswith('t')]].values)\n",
    "        ys = list(window[[col for col in window.index if col.startswith('y')]].values)\n",
    "        zs = list(window[[col for col in window.index if col.startswith('z')]].values)\n",
    "\n",
    "        window_df = {'t': ts, 'y': ys, 'z': zs}\n",
    "        window_df = pd.DataFrame(window_df)\n",
    "        window_df.set_index('t', inplace=True)\n",
    "\n",
    "        # Perform PCA on the data\n",
    "        pca = PCA(n_components=1)  # Set the number of components to 2\n",
    "        principal_components = pca.fit_transform(window_df)\n",
    "\n",
    "        # Create new columns for aPC1 and aPC2 in the DataFrame\n",
    "        pc2.append(principal_components[:, 0])\n",
    "\n",
    "    assert(len(pc2) == len(df))\n",
    "    return pc2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def calculate_pcayz(window):\n",
    "    window_df = {}\n",
    "    ts = list(window[[col for col in window.index if col.startswith('t')]].values)\n",
    "    ys = list(window[[col for col in window.index if col.startswith('y')]].values)\n",
    "    zs = list(window[[col for col in window.index if col.startswith('z')]].values)\n",
    "\n",
    "    window_df = {'t': ts, 'y': ys, 'z': zs}\n",
    "    window_df = pd.DataFrame(window_df)\n",
    "    window_df.set_index('t', inplace=True)\n",
    "\n",
    "    pca = PCA(n_components=1)\n",
    "    principal_components = pca.fit_transform(window_df)\n",
    "    return principal_components\n",
    "\n",
    "def get_pc2(df):\n",
    "    dim_cols = [col for col in df.columns if col.startswith('y') or col.startswith('z') or col.startswith('t')]\n",
    "    pc1 = Parallel(n_jobs=-1)(delayed(calculate_pcayz)(row[dim_cols]) for w, row in df.iterrows())\n",
    "    assert len(pc1) == len(df)\n",
    "    return pc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yzpc = get_pc2(df_gyroscope)\n",
    "print(len(yzpc[0]))\n",
    "print(len(df_gyroscope))\n",
    "print(len(yzpc))\n",
    "\n",
    "add_pc_cols(df_gyroscope, yzpc, 'yzpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yzpc = get_pc2(df_accelerometer)\n",
    "print(len(yzpc[0]))\n",
    "print(len(df_accelerometer))\n",
    "print(len(yzpc))\n",
    "\n",
    "add_pc_cols(df_accelerometer, yzpc, 'yzpc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings\n",
    "def get_raw_x(data_source):\n",
    "    window_size = 250 # Window size of 5 seconds = 250 points\n",
    "    temp_dict = { 'x_'+str(i): [] for i in range(window_size) }\n",
    "\n",
    "    for activity in activities_to_process:\n",
    "        for set_num, data_activity_set in enumerate(data_source[activity]):\n",
    "            x = data_activity_set[:,1]\n",
    "            overlap = 1 # Overlap of 4.8 seconds\n",
    "            windowed_x = np.array([x[i:i+window_size] for i in range(0, len(x) - window_size + 1, overlap)])\n",
    "            window_row_count = windowed_x.shape[0]\n",
    "            \n",
    "            for i in range(len(windowed_x[0])):\n",
    "                temp_dict['x_'+str(i)].extend([windowed_x[j][i] for j in range(len(windowed_x))])\n",
    "    return temp_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gyroscope_x = get_raw_x(activities_gyroscope_data_dict)\n",
    "accelerometer_x = get_raw_x(activities_accelerometer_data_dict)\n",
    "assert(len(gyroscope_x['x_0']) == len(df_gyroscope))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the raw x columns to df_gyroscope\n",
    "for i in range(len(gyroscope_x.keys())):\n",
    "    df_gyroscope['x_'+str(i)] = gyroscope_x['x_'+str(i)]\n",
    "    df_accelerometer['x_'+str(i)] = accelerometer_x['x_'+str(i)]\n",
    "assert(len(gyroscope_x.keys()) == len([col for col in df_gyroscope.columns if col.startswith('x_')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_accelerometer.columns) == len(df_gyroscope.columns)\n",
    "print(len(df_accelerometer.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_gyroscope.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[col for col in df_accelerometer.columns if col not in df_gyroscope.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make df_gyroscope index into columns\n",
    "df_gyroscope.reset_index(inplace=True)\n",
    "assert(len(df_accelerometer.columns) == len(df_gyroscope.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should square root here instead of later\n",
    "for i in range(250):\n",
    "    df_gyroscope['xmag'+str(i)] = df_gyroscope['x'+str(i)]**2 + df_gyroscope['y'+str(i)]**2 + df_gyroscope['z'+str(i)]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should square root here instead of later\n",
    "for i in range(250):\n",
    "    df_accelerometer['axmag'+str(i)] = df_accelerometer['x'+str(i)]**2 + df_accelerometer['y'+str(i)]**2 + df_accelerometer['z'+str(i)]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_gyroscope.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_accelerometer.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write all column names to file\n",
    "with open('gyroscope_column_names.txt', 'w') as f:\n",
    "    for col in df_gyroscope.columns:\n",
    "        f.write(col + '\\n')\n",
    "\n",
    "with open('accelerometer_column_names.txt', 'w') as f:\n",
    "    for col in df_accelerometer.columns:\n",
    "        f.write(col + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df_gyroscope.columns))\n",
    "print(len(df_accelerometer.columns))\n",
    "assert(len(df_accelerometer.columns) == len(df_gyroscope.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gyroscope.columns = [col[2:] if col.endswith('source') \n",
    "    or col.endswith('set_num') or col.endswith('activity') or col.endswith('rep_counts_actual')\n",
    "    else col for col in df_gyroscope.columns]\n",
    "df_accelerometer.columns = [col[2:] if col.endswith('source') \n",
    "    or col.endswith('set_num') or col.endswith('activity') or col.endswith('rep_counts_actual')\n",
    "    else col for col in df_gyroscope.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_cols = [col for col in df_gyroscope.columns if col.startswith('t')]\n",
    "mutual_cols.extend(['rep_counts_actual'])\n",
    "len(mutual_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gyroscope.to_pickle('df_gyroscope0611.pkl')\n",
    "df_accelerometer.to_pickle('df_accelerometer0611.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join accelerometer and gyroscope dataframes on index\n",
    "df = df_gyroscope.join(df_accelerometer, how='inner', lsuffix='_gyroscope', rsuffix='_accelerometer')\n",
    "assert(len(df_gyroscope) == len(df) == len(df_accelerometer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: should've done this earlier\n",
    "# for column name that contains \"xmag\" in df, replace value with the square root\n",
    "for col in df.columns:\n",
    "    if col.startswith('g_gxmag') or col.startswith('a_axmag'):\n",
    "        df[col] = df[col].apply(lambda x: math.sqrt(x))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation features (computed for each signal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle(\"mergeddf_0611.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# turn activities_gyroscope_data_dict and activities_accelerometer_data_dict into a spark dataframe\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "\n",
    "# create schema for data\n",
    "schema = StructType([\n",
    "    StructField(\"overall_set_num\", IntegerType(), True),\n",
    "    StructField(\"activity_name\", StringType(), True),\n",
    "    StructField(\"activity_set_num\", IntegerType(), True),\n",
    "    StructField(\"time\", DoubleType(), True),\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"z\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# create empty dataframes\n",
    "gyroscope_df = ss.createDataFrame(sc.emptyRDD(), schema)\n",
    "accelerometer_df = ss.createDataFrame(sc.emptyRDD(), schema)\n",
    "\n",
    "# iterate over activities and append data to dataframes\n",
    "overall_set_num = 0\n",
    "for activity in activities_to_process:\n",
    "    for activity_set_num, activity_set in enumerate(activities_gyroscope_data_dict[activity]):\n",
    "        for time_point in activity_set:\n",
    "            data_row = [(overall_set_num, str(activity), activity_set_num, float(time_point[0]), float(time_point[1]), float(time_point[2]), float(time_point[3]))]\n",
    "            gyroscope_df = gyroscope_df.union(ss.createDataFrame(data_row, schema))\n",
    "        overall_set_num += 1\n",
    "    overall_set_num -= activity_set_num\n",
    "    for activity_set_num, activity_set in enumerate(activities_accelerometer_data_dict[activity]):\n",
    "        for time_point in activity_set:\n",
    "            data_row = [(overall_set_num, str(activity), activity_set_num, float(time_point[0]), float(time_point[1]), float(time_point[2]), float(time_point[3]))]\n",
    "            accelerometer_df = accelerometer_df.union(ss.createDataFrame(data_row, schema))\n",
    "        overall_set_num += 1\n",
    "\n",
    "\n",
    "# show dataframes \n",
    "gyroscope_df.show()\n",
    "accelerometer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# turn activities_gyroscope_data_dict and activities_accelerometer_data_dict into a spark dataframe\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "sc = ss.sparkContext\n",
    "\n",
    "# create schema for data\n",
    "schema = StructType([\n",
    "    StructField(\"overall_set_num\", IntegerType(), True),\n",
    "    StructField(\"activity_name\", StringType(), True),\n",
    "    StructField(\"activity_set_num\", IntegerType(), True),\n",
    "    StructField(\"time\", DoubleType(), True),\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"z\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# create empty dataframes\n",
    "gyroscope_df = ss.createDataFrame(sc.emptyRDD(), schema)\n",
    "accelerometer_df = ss.createDataFrame(sc.emptyRDD(), schema)\n",
    "\n",
    "gyroscope_data = []  \n",
    "accelerometer_data = [] \n",
    "\n",
    "# Iterate over activities and append data to dataframes\n",
    "overall_set_num = 0\n",
    "for activity in activities_to_process:\n",
    "    for activity_set_num, activity_set in enumerate(activities_gyroscope_data_dict[activity]):\n",
    "        for time_point in activity_set:\n",
    "            data_row = (overall_set_num, str(activity), activity_set_num, float(time_point[0]), float(time_point[1]), float(time_point[2]), float(time_point[3]))\n",
    "            gyroscope_data.append(data_row)\n",
    "        overall_set_num += 1\n",
    "    overall_set_num -= activity_set_num\n",
    "    for activity_set_num, activity_set in enumerate(activities_accelerometer_data_dict[activity]):\n",
    "        for time_point in activity_set:\n",
    "            data_row = (overall_set_num, str(activity), activity_set_num, float(time_point[0]), float(time_point[1]), float(time_point[2]), float(time_point[3]))\n",
    "            accelerometer_data.append(data_row)\n",
    "        overall_set_num += 1\n",
    "\n",
    "# Create dataframes directly from the lists of data rows\n",
    "gyroscope_df = ss.createDataFrame(gyroscope_data, schema)\n",
    "accelerometer_df = ss.createDataFrame(accelerometer_data, schema)\n",
    "\n",
    "# Show dataframes\n",
    "gyroscope_df.show()\n",
    "accelerometer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create empty lists to collect data points\n",
    "gyroscope_data = []\n",
    "accelerometer_data = []\n",
    "\n",
    "overall_set_num = 0\n",
    "for activity in activities_to_process:\n",
    "    for activity_set_num, activity_set in enumerate(activities_gyroscope_data_dict[activity]):\n",
    "        for time_point in activity_set:\n",
    "            data_row = (overall_set_num, str(activity), activity_set_num, float(time_point[0]), float(time_point[1]), float(time_point[2]), float(time_point[3]))\n",
    "            gyroscope_data.append(data_row)\n",
    "        overall_set_num += 1\n",
    "    overall_set_num -= activity_set_num\n",
    "    for activity_set_num, activity_set in enumerate(activities_accelerometer_data_dict[activity]):\n",
    "        for time_point in activity_set:\n",
    "            data_row = (overall_set_num, str(activity), activity_set_num, float(time_point[0]), float(time_point[1]), float(time_point[2]), float(time_point[3]))\n",
    "            accelerometer_data.append(data_row)\n",
    "        overall_set_num += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gyroscope_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# turn activities_gyroscope_data_dict and activities_accelerometer_data_dict into a spark dataframe\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "# sc = ss.sparkContext\n",
    "\n",
    "# spark = SparkSession.builder.getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rdd = ss.sparkContext.parallelize(gyroscope_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"overall_set_num\", IntegerType(), True),\n",
    "    StructField(\"activity_name\", StringType(), True),\n",
    "    StructField(\"activity_set_num\", IntegerType(), True),\n",
    "    StructField(\"time\", DoubleType(), True),\n",
    "    StructField(\"x\", DoubleType(), True),\n",
    "    StructField(\"y\", DoubleType(), True),\n",
    "    StructField(\"z\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data_rdd, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create a SparkSession\n",
    "ss = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Define the schema for gyroscope and accelerometer data\n",
    "schema = StructType([\n",
    "    StructField(\"timestamp\", LongType(), nullable=False),\n",
    "    StructField(\"x\", DoubleType(), nullable=False),\n",
    "    StructField(\"y\", DoubleType(), nullable=False),\n",
    "    StructField(\"z\", DoubleType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Convert gyroscope_data and accelerometer_data to a list of rows\n",
    "gyroscope_rows = [(k, v[0], v[1], v[2]) for k, v in activities_gyroscope_data_dict.items()]\n",
    "accelerometer_rows = [(k, v[0], v[1], v[2]) for k, v in activities_accelerometer_data_dict.items()]\n",
    "\n",
    "# Create DataFrames from the collected data\n",
    "gyroscope_df = ss.createDataFrame(gyroscope_rows, schema)\n",
    "accelerometer_df = ss.createDataFrame(accelerometer_rows, schema)\n",
    "\n",
    "# Show the DataFrames\n",
    "gyroscope_df.show()\n",
    "accelerometer_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca4bdd2ec3710d32cba740d0f2a8504a4cffd42a76bfba64146e079be5d25276"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
